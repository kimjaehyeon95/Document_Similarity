{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "import csv\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\KJH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\KJH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\KJH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\KJH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 본문 텍스트 추출\n",
    "def parsing(par1):\n",
    "    last = []\n",
    "    l_style =[]\n",
    "    i = 0\n",
    "    c = 0\n",
    "    text = \"\"\n",
    "\n",
    "    #p 속성 파싱\n",
    "    for a in par1:\n",
    "        p_style = a.style.name\n",
    "        p_bold = a.style.font.bold\n",
    "        p_font = a.style.font.name\n",
    "        p_size = a.style.font.size\n",
    "        if p_size != None:\n",
    "            p_size = p_size.pt   \n",
    "        p_italic = a.style.font.italic\n",
    "        p_underline = a.style.font.underline\n",
    "\n",
    "        #r속성 파싱\n",
    "        for test in a.runs:\n",
    "            if (test.text.isspace() == False) and (test.text != \"\"):\n",
    "                last.append([])\n",
    "\n",
    "                #소문자 변경\n",
    "                text = test.text.lower()\n",
    "\n",
    "                #한글/영어 제외 제거\n",
    "                text = re.compile('[^ ㄱ-ㅣ가-힣|a-z]+').sub('', text)\n",
    "\n",
    "                last[i].append(text)\n",
    "\n",
    "\n",
    "                #글자 속성 파싱(굵게)\n",
    "                if test.bold == None:\n",
    "                    if test.style.name == \"Default Paragraph Font\":\n",
    "                        last[i].append(p_bold)\n",
    "                    else:\n",
    "                        last[i].append(test.style.font.bold)\n",
    "                else:\n",
    "                    last[i].append(test.bold)\n",
    "\n",
    "\n",
    "                #글자 속성 파싱(글꼴)\n",
    "                if test.font.name == None:\n",
    "                    last[i].append(p_font)\n",
    "                else:\n",
    "                    last[i].append(test.font.name)\n",
    "\n",
    "                #글자 속성 파싱(글자 크기)\n",
    "                if test.font.size == None:\n",
    "                    if test.style.name == \"Default Paragraph Font\":\n",
    "                        if p_size != None:\n",
    "                            last[i].append(p_size)\n",
    "                        else:\n",
    "                            last[i].append(float(10))\n",
    "                    elif test.style.font.size != None:\n",
    "                        last[i].append(test.style.font.size.pt)\n",
    "                    else:\n",
    "                        last[i].append(float(10))\n",
    "\n",
    "                else:\n",
    "                    last[i].append(test.font.size.pt)\n",
    "\n",
    "                #글자 속성 파싱(기울임 꼴)\n",
    "                if test.italic == None:\n",
    "                    if test.style.name == \"Default Paragraph Font\":\n",
    "                        last[i].append(p_italic)\n",
    "                    else:\n",
    "                        last[i].append(test.style.font.italic)\n",
    "                else:\n",
    "                    last[i].append(test.italic)\n",
    "\n",
    "\n",
    "                #글자 속성 파싱(밑줄)\n",
    "                if test.underline == None:\n",
    "                    if test.style.name == \"Default Paragraph Font\":\n",
    "                        last[i].append(p_underline)\n",
    "                    else:\n",
    "                        last[i].append(test.style.font.underline)\n",
    "                else:\n",
    "                    last[i].append(test.underline)\n",
    "\n",
    "                #글자 속성 파싱(스타일)\n",
    "                if test.style.name == \"Default Paragraph Font\":\n",
    "                    last[i].append(p_style)\n",
    "                    l_style.append(p_style)\n",
    "                else:\n",
    "                    last[i].append(test.style.name)\n",
    "                    l_style.append(test.style.name)\n",
    "\n",
    "                c = i\n",
    "                i= i+1\n",
    "\n",
    "        if len(a.runs)!= 0 and (text.isspace() == False) and (text != \"\"):\n",
    "            last[c][0] = last[c][0] +\" \"\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    return last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#노이즈 제거 (문서 작성 시 사용되는 다중 띄어쓰기 등)\n",
    "def Dedupli(last):\n",
    "    i = 0\n",
    "    j = 0\n",
    "\n",
    "\n",
    "\n",
    "    while i < len(last)-1:\n",
    "        if i + j >= len(last)-1:\n",
    "            break\n",
    "\n",
    "        for j in range(1, len(last)-i):\n",
    "            if last[i][1] == last[i+j][1] and last[i][2] == last[i+j][2] and last[i][3] == last[i+j][3] and last[i][4] == last[i+j][4] and last[i][5] == last[i+j][5] and last[i][6] == last[i+j][6]:\n",
    "                last[i][0] = last[i][0] + last[i+j][0]\n",
    "                last[i+j][0] =\"\"\n",
    "            else:\n",
    "                i = i+j\n",
    "                break\n",
    "\n",
    "    \n",
    "    real_last = [[]]\n",
    "\n",
    "    for text in last[:]:\n",
    "        a = text[0].isspace()\n",
    "        if (a==True) or (text[0] == \"\"):\n",
    "            del text\n",
    "        elif (len(text[0]) == 1):\n",
    "            del text\n",
    "        else:\n",
    "            real_last.append(text)\n",
    "\n",
    "\n",
    "    return real_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전처리(영어 어간 추출)\n",
    "def pret(texts):\n",
    "    stops = set(stopwords.words('english'))\n",
    "    no_stops = []\n",
    "    a = []\n",
    "    \n",
    "    l = len(texts)\n",
    "    i = 1\n",
    "    c = 0\n",
    "    stemmer = PorterStemmer()\n",
    "    text_en = []\n",
    "\n",
    "\n",
    "\n",
    "    for j in range(len(texts)-1):\n",
    "        a = text_to_word_sequence(texts[i][0])\n",
    "        for t in a:\n",
    "            if t not in stops:\n",
    "                no_stops.append(t)\n",
    "\n",
    "        #text_en = arche_type(no_stops)\n",
    "        for c in range(0,len(no_stops)):\n",
    "            text_en.append(stemmer.stem(no_stops[c]))\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        texts[i][0] = \" \".join(map(str, text_en))\n",
    "        i = i+1\n",
    "        no_stops = []\n",
    "        text_en = []\n",
    "\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CSV 용 추가사항\n",
    "def pret_plus(real_last):\n",
    "    for i in range(1, len(real_last)):\n",
    "            if real_last[i][1] != True:\n",
    "                real_last[i][1] = \"None\"\n",
    "\n",
    "            if real_last[i][4] != True:\n",
    "                real_last[i][4] = \"None\"\n",
    "\n",
    "            if real_last[i][5] != True:\n",
    "                real_last[i][5] = \"None\"\n",
    "\n",
    "            if real_last[i][2] == None:\n",
    "                real_last[i][2] = \"기본 글꼴\"\n",
    "    return real_last\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#품사 태깅 및 매칭\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    if pos_tag.startswith('V'):\n",
    "        return 'v'\n",
    "    if pos_tag.startswith('N'):\n",
    "        return 'n'\n",
    "    elif pos_tag.startswith('J'):\n",
    "        return 'a'\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#텍스트 원형 복원\n",
    "def arche_type(test_text):\n",
    "\n",
    "    tokens_pos_list = pos_tag(test_text)\n",
    "\n",
    "\n",
    "\n",
    "    tokens_pos_list2 = []\n",
    "    for token_list in tokens_pos_list:\n",
    "        temp_list = []\n",
    "        (token, pos_tags) = token_list\n",
    "        tag = get_wordnet_pos(pos_tags)\n",
    "        if tag!=None:\n",
    "            temp_list.append((token, get_wordnet_pos(pos_tags)))\n",
    "        tokens_pos_list2.append(temp_list)\n",
    "\n",
    "\n",
    "    lemm = WordNetLemmatizer()\n",
    "    token_final = []\n",
    "    for token_pos in tokens_pos_list2:\n",
    "        for token, tag in token_pos:\n",
    "            token_final.append(lemm.lemmatize(token, pos=tag))\n",
    "\n",
    "    return token_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#가중치 함수\n",
    "def weighting(real_last):\n",
    "    all_len = 0\n",
    "    n_bol_len =0\n",
    "    bol_len = 0\n",
    "    n_inc_len =0\n",
    "    inc_len =0\n",
    "    size_len =[]\n",
    "    n_und_len =0\n",
    "    und_len =0\n",
    "\n",
    "    #토큰수에 따른 비율 설정\n",
    "    for i in range(1, len(real_last)):\n",
    "        tok_len = len(text_to_word_sequence(real_last[i][0]))\n",
    "        real_last[i].append(tok_len)\n",
    "\n",
    "        if real_last[i][3] != None:\n",
    "            for j in range(tok_len):\n",
    "                size_len.append(int(real_last[i][3]))\n",
    "        else:\n",
    "            for j in range(tok_len):\n",
    "                size_len.append(int(10))\n",
    "\n",
    "        if real_last[i][4] == True:\n",
    "            inc_len = inc_len + tok_len\n",
    "        else:\n",
    "            n_inc_len = n_inc_len + tok_len\n",
    "\n",
    "        if real_last[i][5] == True:\n",
    "            und_len = und_len + tok_len\n",
    "        else:\n",
    "            n_und_len = n_und_len + tok_len\n",
    "\n",
    "        if real_last[i][1] == True:\n",
    "            bol_len = bol_len + tok_len\n",
    "        else:\n",
    "            n_bol_len = n_bol_len + tok_len\n",
    "\n",
    "\n",
    "        all_len = all_len + tok_len\n",
    "\n",
    "    std = np.std(size_len)\n",
    "    mean = np.mean(size_len)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #가중치 설정\n",
    "    for i in range(1, len(real_last)):\n",
    "        #기본값\n",
    "        a = 0.2\n",
    "        if real_last[i][1] == True:\n",
    "            a = a+(n_bol_len/all_len)\n",
    "        else:\n",
    "            a = a+(bol_len/all_len)\n",
    "        \n",
    "        if std != 0:\n",
    "            w = float((real_last[i][3]-mean)/mean)\n",
    "            a = a+w\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        if real_last[i][4] == True:\n",
    "            a = a+(n_inc_len/all_len)\n",
    "        else:\n",
    "            a = a+(inc_len/all_len)\n",
    "\n",
    "        if real_last[i][5] == True:\n",
    "            a = a+(n_und_len/all_len)\n",
    "        else:\n",
    "            a = a+(und_len/all_len)\n",
    "\n",
    "\n",
    "        real_last[i].append(a)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return real_last\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#코사인 값 함수\n",
    "def cos_sim(a, b):\n",
    "    return dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "#가중치 추가한 빈도수 함수\n",
    "def make_matrix(feats, list_data , we):\n",
    "    freq_list = []\n",
    "    data = []\n",
    "    tok_we = []\n",
    "    j = 0\n",
    "    for text in list_data:\n",
    "        tok = text_to_word_sequence(text)\n",
    "        data = data+tok        \n",
    "        for i in range(len(tok)):\n",
    "            tok_we.append(we[j])\n",
    "        j +=1\n",
    "\n",
    "    for feat in feats:\n",
    "        freq = 0\n",
    "        t = 0\n",
    "        for word in data:\n",
    "            if feat == word:\n",
    "                freq += tok_we[t]\n",
    "            t+=1\n",
    "        freq_list.append(freq)\n",
    "\n",
    "\n",
    "    return freq_list\n",
    "\n",
    "#기존 코사인 알고리즘을 위한 빈도 행렬 함수\n",
    "def make_matrix2(feats, list_data):\n",
    "    freq_list = []\n",
    "    for feat in feats:\n",
    "        freq = 0\n",
    "        for word in list_data:\n",
    "            if feat == word:\n",
    "                freq += 1\n",
    "        freq_list.append(freq)\n",
    "    return freq_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#본문 텍스트 및 가중치\n",
    "def dataization(real_last):\n",
    "    i = 1\n",
    "    j = 1\n",
    "    test1 = []\n",
    "    test1.append([])\n",
    "    test1.append([])\n",
    "\n",
    "    for j in range(len(real_last)-1):\n",
    "        if len(real_last[i][0]) > 1 :\n",
    "            test1[0].append(real_last[i][0])\n",
    "            test1[1].append(real_last[i][8])\n",
    "            i += 1\n",
    "\n",
    "    return test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#파싱 및 전처리 과정 합친 함수(실험 1, 문서 분할)\n",
    "def all_parsing(d):\n",
    "    doc = Document(d)\n",
    "    Num_par = len(doc.paragraphs)\n",
    "    par1 = doc.paragraphs\n",
    "\n",
    "    test = parsing(par1)\n",
    "    test_last = Dedupli(test)    \n",
    "    test_last = pret(test_last)   \n",
    "    test_last = weighting(test_last)\n",
    "\n",
    "\n",
    "    test_last = dataization(test_last)\n",
    "    \n",
    "    first_array = []\n",
    "    first_array.append([])\n",
    "    first_array.append([])\n",
    "    second_array = []\n",
    "    second_array.append([])\n",
    "    second_array.append([])\n",
    "\n",
    "    e = 0\n",
    "\n",
    "    for e in range(len(test_last[0])):\n",
    "        if e%4 == 3:\n",
    "            first_array[0].append(test_last[0][e])\n",
    "            first_array[1].append(test_last[1][e])\n",
    "\n",
    "        else:\n",
    "            second_array[0].append(test_last[0][e])\n",
    "            second_array[1].append(test_last[1][e])\n",
    "\n",
    "    return first_array, second_array\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#파싱 및 전처리(동일서식 확인)\n",
    "def all_parsing2(d):\n",
    "    doc = Document(d)\n",
    "    par1 = doc.paragraphs\n",
    "\n",
    "    test = parsing(par1)\n",
    "    test_last = Dedupli(test)    \n",
    "    test_last = pret(test_last)   \n",
    "    test_last = weighting(test_last)\n",
    "\n",
    "\n",
    "    test_last = dataization(test_last)\n",
    "    \n",
    "    return test_last\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#실험 유사도 측정 함수\n",
    "def test_fuction(path):\n",
    "    a,b = all_parsing(path)\n",
    "\n",
    "\n",
    "    z1 = \" \".join(s for s in a[0])\n",
    "    z2 = \" \".join(s for s in b[0])\n",
    "\n",
    "\n",
    "    t1 = text_to_word_sequence(z1)\n",
    "    t2 = text_to_word_sequence(z2)\n",
    "\n",
    "\n",
    "    feat2 = set(t1+t2)\n",
    "\n",
    "    #가중치를 추가한 각 빈도\n",
    "\n",
    "\n",
    "    v1 = np.array(make_matrix(feat2, a[0], a[1]))\n",
    "    v2 = np.array(make_matrix(feat2, b[0], b[1]))\n",
    "\n",
    "    cs = cos_sim(v1, v2)\n",
    "\n",
    "\n",
    "    feats2 = set(t1+ t2)\n",
    "\n",
    "\n",
    "    v3 = np.array(make_matrix2(feats2, t1))\n",
    "    v4 = np.array(make_matrix2(feats2, t2))\n",
    "\n",
    "\n",
    "    cs1 = cos_sim(v3, v4)\n",
    "\n",
    "    print(\"name : \" + str(path) + \" 가중치 : \" +str(cs) +\" 기존 : \" + str(cs1))\n",
    "\n",
    "    return cs, cs1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/15 [00:02<00:28,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : C:/Users/KJH/Desktop/last_folder/test2/119.docx 가중치 : 0.7896690344803345 기존 : 0.766463601704344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2/15 [00:05<00:35,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : C:/Users/KJH/Desktop/last_folder/test2/14.docx 가중치 : 0.8983551415662834 기존 : 0.8849295319684343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3/15 [00:06<00:25,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : C:/Users/KJH/Desktop/last_folder/test2/208.docx 가중치 : 0.7340747912076461 기존 : 0.6853470084911195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 4/15 [00:10<00:31,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : C:/Users/KJH/Desktop/last_folder/test2/408.docx 가중치 : 0.9166959037673234 기존 : 0.9028125034552947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5/15 [00:13<00:28,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : C:/Users/KJH/Desktop/last_folder/test2/529.docx 가중치 : 0.842681619351202 기존 : 0.8253605900974575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 6/15 [00:14<00:19,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : C:/Users/KJH/Desktop/last_folder/test2/537.docx 가중치 : 0.5274860039754535 기존 : 0.3026735752767626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7/15 [00:14<00:12,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : C:/Users/KJH/Desktop/last_folder/test2/66.docx 가중치 : 0.7140389891877551 기존 : 0.6885683554949162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 8/15 [00:17<00:14,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : C:/Users/KJH/Desktop/last_folder/test2/dataset[learn]_154.docx 가중치 : 0.5834585985263623 기존 : 0.577247048819141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 9/15 [00:18<00:10,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : C:/Users/KJH/Desktop/last_folder/test2/dataset[learn]_187.docx 가중치 : 0.9060564310544054 기존 : 0.8851071682685056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 10/15 [00:24<00:14,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : C:/Users/KJH/Desktop/last_folder/test2/dataset[learn]_248.docx 가중치 : 0.5062768656319824 기존 : 0.4286758823712451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 11/15 [00:24<00:08,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : C:/Users/KJH/Desktop/last_folder/test2/dataset[learn]_293.docx 가중치 : 0.7650809902906686 기존 : 0.807735269308121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 13/15 [00:26<00:02,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : C:/Users/KJH/Desktop/last_folder/test2/dataset[learn]_33.docx 가중치 : 0.8565402819146628 기존 : 0.8563125583741192\n",
      "name : C:/Users/KJH/Desktop/last_folder/test2/dataset[learn]_353.docx 가중치 : 0.6339919326124303 기존 : 0.6429788582206545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 14/15 [00:28<00:01,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : C:/Users/KJH/Desktop/last_folder/test2/dataset[learn]_411.docx 가중치 : 0.687493299139179 기존 : 0.6130911208226802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:31<00:00,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : C:/Users/KJH/Desktop/last_folder/test2/dataset[learn]_723.docx 가중치 : 0.8210409788652903 기존 : 0.826251015043785\n",
      "가중치 평균0.7455293907713987\n",
      "기존 평균0.7129036058477721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#가중치 및 기존함수 비교\n",
    "\n",
    "path = 'C:/Users/KJH/Desktop/last_folder/test2'\n",
    "\n",
    "e = []\n",
    "f = []\n",
    "\n",
    "files = os.listdir(path)\n",
    "\n",
    "for i in range(len(files)):\n",
    "    files[i] = 'C:/Users/KJH/Desktop/last_folder/test2/' + files[i]\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(files))):\n",
    "    c,d = test_fuction(files[i])\n",
    "    e.append(c)\n",
    "    f.append(d)\n",
    "\n",
    "\n",
    "print(\"가중치 평균\" +str(sum(e)/len(e)))\n",
    "print(\"기존 평균\" + str(sum(f)/len(f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name :  가중치 : 0.9999830884367636 기존 : 0.9999999999999998\n"
     ]
    }
   ],
   "source": [
    "#실험 2\n",
    "doc1 = 'C:/Users/KJH/Desktop/docx테스트/최종데이터셋/1/테스트4.docx'\n",
    "doc2 = 'C:/Users/KJH/Desktop/docx테스트/최종데이터셋/1/테스트원본.docx'\n",
    "\n",
    "c = all_parsing2(doc1)\n",
    "d = all_parsing2(doc2)\n",
    "\n",
    "\n",
    "z1 = \" \".join(s for s in c[0])\n",
    "z2 = \" \".join(s for s in d[0])\n",
    "\n",
    "\n",
    "t1 = text_to_word_sequence(z1)\n",
    "t2 = text_to_word_sequence(z2)\n",
    "\n",
    "\n",
    "feat2 = set(t1+t2)\n",
    "\n",
    "#가중치를 추가한 각 빈도\n",
    "\n",
    "\n",
    "v1 = np.array(make_matrix(feat2, c[0], c[1]))\n",
    "v2 = np.array(make_matrix(feat2, d[0], d[1]))\n",
    "\n",
    "cs = cos_sim(v1, v2)\n",
    "\n",
    "\n",
    "feats2 = set(t1+ t2)\n",
    "\n",
    "\n",
    "v3 = np.array(make_matrix2(feats2, t1))\n",
    "v4 = np.array(make_matrix2(feats2, t2))\n",
    "\n",
    "\n",
    "cs1 = cos_sim(v3, v4)\n",
    "\n",
    "print(\"name :  가중치 : \" +str(cs) +\" 기존 : \" + str(cs1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#확인용 csv 파일 생성\n",
    "fields = ['Text', 'Bold', 'Font', 'Size', 'Italic', 'Underline','Style','S_Bold', 'S_Size', 'S_Italic', 'S_Underline']\n",
    "\n",
    "d = \"C:/Users/KJH/Desktop/last_folder/test3/Community.docx\"\n",
    "\n",
    "\n",
    "doc = Document(d)\n",
    "Num_par = len(doc.paragraphs)\n",
    "par1 = doc.paragraphs\n",
    "\n",
    "test = parsing(par1)\n",
    "test_last = Dedupli(test)    \n",
    "test_last = pret(test_last)   \n",
    "test_last = pret_plus(test_last)\n",
    "\n",
    "\n",
    "\n",
    "with open('case1.csv','w',newline='',encoding='utf-8-sig') as file :\n",
    "\n",
    "    write = csv.writer(file)\n",
    "    write.writerow(fields)\n",
    "    \n",
    "\n",
    "    for i in range(len(test_last)):\n",
    "        write.writerow(test_last[i])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ded6559c88b47b777732012ac42d40694be81dd0b881a7962c11e050b83f61d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
